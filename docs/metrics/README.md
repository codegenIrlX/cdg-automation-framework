# Документация: Сбор и запись метрик в фреймворке автотестирования

---

## Оглавление

- [1. Введение](#1-введение)
  - [Зачем собирать метрики в автотестах](#зачем-собирать-метрики-в-автотестах)
  - [Как метрики помогают в мониторинге, отладке, CI/CD, SLA](#как-метрики-помогают-в-мониторинге-отладке-cicd-sla)
  - [Какие компоненты фреймворка должны генерировать метрики](#какие-компоненты-фреймворка-должны-генерировать-метрики)
- [2. Структура и формат метрик](#2-структура-и-формат-метрик)
  - [Рекомендуемый формат — jsonl (JSON Lines)](#рекомендуемый-формат--jsonl-json-lines)
  - [Пример строки метрики (jsonl)](#пример-строки-метрики-jsonl)
  - [Альтернативы и почему jsonl предпочтителен](#альтернативы-и-почему-jsonl-предпочтителен)
- [3. Что собирать: список метрик](#3-что-собирать-список-метрик)
- [4. Реализация в коде](#4-реализация-в-коде)
  - [Принцип включения/отключения](#принцип-включенияотключения)
  - [Где и как собирать метрики](#где-и-как-собирать-метрики)
  - [Пример функции логирования метрик (минимальный)](#пример-функции-логирования-метрик-минимальный)
  - [Рекомендации для реальных проектов](#рекомендации-для-реальных-проектов)
- [5. Сбор метрик из Kafka](#5-сбор-метрик-из-kafka)
- [6. Сбор метрик из RabbitMQ](#6-сбор-метрик-из-rabbitmq)
- [7. Сбор метрик из БД](#7-сбор-метрик-из-бд)
- [8. Best Practices](#8-best-practices)
- [9. Заключение](#9-заключение)
- [Приложение: рекомендуемая схема полей](#приложение-рекомендуемая-схема-полей)

---


## 1. Введение

### Зачем собирать метрики в автотестах

Метрики в автотестах — это структурированные события о том, **что именно произошло** во время запуска теста и **сколько это заняло времени**. Они дополняют обычные логи и отчёты, потому что:
- их легко агрегировать, фильтровать и анализировать;
- они стабильно читаются машиной;
- по ним можно строить дашборды и алерты.

### Как метрики помогают в мониторинге, отладке, CI/CD, SLA

- **Мониторинг стабильности:** рост ошибок по `http_status`, увеличение `duration_ms`, нестабильность по окружениям.
- **Отладка:** быстро связать тест → запрос → событие Kafka → запрос в БД через `request_id`.
- **CI/CD:** сравнение прогонов (до/после релиза), регресс времени ответа, “красные” тесты, флапающие тесты.
- **SLA/Perf regressions:** выявление деградаций по p95/p99 задержек (например, API или обработка сообщений).
- **Аналитика качества тестов:** какие тесты “тяжёлые”, где чаще всего ломается, какие окружения нестабильны.

### Какие компоненты фреймворка должны генерировать метрики

Рекомендуемая зона ответственности:
- **Тестовый раннер / фикстуры**: `test_name`, `environment`, `timestamp`, генерация `request_id` и “корневого контекста”.
- **API-клиент (BaseClient)**: `method`, `endpoint`, `http_status`, `duration_ms`, `request_id`.
- **Kafka/MQ-клиент**: `topic`, `partition`, `offset`, `message_id`, `processing_time_ms`, связь с `request_id`.
- **DB-слой (SQLAlchemy / session wrapper)**: `query_type`, `table`, `duration_ms`, `rowcount`, `request_id`.

---

## 2. Структура и формат метрик

### Рекомендуемый формат — **jsonl** (JSON Lines)

**jsonl** — это файл, где **каждая строка** является отдельным JSON-объектом.

Почему это удобно:
- **Лёгкость обработки**: построчное чтение без загрузки всего файла в память.
- **Совместимость**: легко отправлять в Loki/Elastic/S3/Minio, парсить в Spark, ClickHouse, Python, jq и т. д.
- **Надёжность**: если запись прервана, не “ломается” весь файл, как бывает с единым JSON-массивом.
- **Простота**: минимальные зависимости, можно вести локально и как артефакт CI.

### Пример строки метрики (jsonl)

```json
{"test_name":"test_get_balance","status":"passed","method":"GET","endpoint":"/api/v1/payment/balance","request_id":"4fa82c09-1234-45bb-9d9f-aaa2f2c9d5ee","http_status":200,"duration_ms":253,"environment":"stage","timestamp":"2024-01-01T12:00:00Z"}
```

> На практике в jsonl **не используется многострочный JSON** — одна строка = одно событие.

### Альтернативы и почему jsonl предпочтителен

#### CSV
**Плюсы:** человекочитаемость, простая загрузка в табличные инструменты.  
**Минусы:** сложно хранить вложенные структуры, нужно заранее фиксировать схему колонок, проблемы с экранированием.  
**Вывод:** подходит для отчётов, но хуже как основной формат событий.

#### InfluxDB / TimescaleDB
**Плюсы:** классика time-series, метрики в виде временных рядов.  
**Минусы:** требует отдельной инфраструктуры и протоколов записи, усложняет локальный запуск и CI.  
**Вывод:** хорошо для прод-мониторинга, но избыточно для базового уровня фреймворка.

#### Prometheus
**Плюсы:** стандарт индустрии для метрик, удобные дашборды Grafana.  
**Минусы:** Prometheus не предназначен для “event log” по каждому тестовому шагу; контекст (`request_id`, `endpoint`) быстро приводит к высокой кардинальности лейблов.  
**Вывод:** лучше как следующий шаг (агрегация + exporter/pushgateway), а не как первичный формат событий фреймворка.

**jsonl** даёт лучший баланс: простая запись в фреймворке + гибкая выгрузка “куда угодно” позже.

---

## 3. Что собирать: список метрик

Базовый набор (универсален для API/Kafka/DB):

- `test_name` — название теста или шага
- `method` — HTTP-метод запроса (GET/PUT/POST/DELETE)
- `endpoint` — URL без домена (например, `/api/v1/payment/balance`)
- `request_id` — сквозной ID, связывающий API, Kafka, БД
- `http_status` — ответ сервера
- `duration_ms` — время выполнения запроса/шага
- `environment` — окружение запуска (dev/stage/prod/local)
- `timestamp` — время события (ISO 8601, UTC)
- *(опционально)* `source` — источник метрики: `api`, `kafka`, `db`, `test`

Рекомендуемые дополнительные поля (по мере зрелости):
- `suite` / `feature` / `story` — группировка (например, из Allure labels)
- `build_id` / `pipeline_id` / `git_sha` — связь с CI
- `status` — `passed/failed/broken/skipped`
- `error_type` / `error_message` — только безопасные, без чувствительных данных
- `retries` — число повторов (если реализованы ретраи)

---

## 4. Реализация в коде

### Принцип включения/отключения

Метрики должны быть **управляемыми**, чтобы не мешать локальному дебагу и не раздувать артефакты.

Рекомендуемые переменные окружения:
- `METRICS_ENABLED=true|false` (по умолчанию `false`)
- `METRICS_PATH=./artifacts/metrics/metrics.jsonl`
- `ENVIRONMENT=local|dev|stage|prod`

### Где и как собирать метрики

#### 4.1 На уровне API-клиента (BaseClient)

Сбор “в одном месте” даёт:
- единый формат;
- автоматическое измерение `duration_ms`;
- прозрачную интеграцию для доменных сервисов.

Рекомендуемый поток:
1. `BaseClient.request(...)` измеряет длительность.
2. Достаёт `request_id` из контекста (fixture/contextvar).
3. Пишет строку jsonl через `MetricsLogger`.

#### 4.2 Через декораторы/фикстуры вокруг тестов

Задачи тестового слоя:
- сгенерировать `request_id` (корреляционный идентификатор) и положить в контекст;
- записать “метрику теста” (старт/финиш, `status` = `passed`/`failed`);
- прикрепить `metrics.jsonl` в Allure (attachment) при необходимости.

#### 4.3 Middleware-подход для прозрачной интеграции

Если API-клиент построен на `httpx`/`requests`, можно использовать hooks/middleware, чтобы:
- не дублировать тайминг;
- единообразно логировать все запросы.

---

### Пример функции логирования метрик (минимальный)

```python
import json
from datetime import datetime, timezone

def log_metric(data: dict, file_path: str = "metrics.jsonl") -> None:
    data = dict(data)
    data.setdefault("timestamp", datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"))

    with open(file_path, "a", encoding="utf-8") as f:
        f.write(json.dumps(data, ensure_ascii=False) + "\n")
```

### Рекомендации для реальных проектов

- Для `pytest-xdist` — **файл на воркер** (`metrics_gw0.jsonl`, `metrics_gw1.jsonl`) и merge после прогона.
- Для потоков — использовать **lock** на запись.
- Для стабильной схемы — валидировать через **Pydantic** (опционально), чтобы не “плыла” структура.

---

## 5. Сбор метрик из Kafka

### Пример: логирование события Kafka в момент потребления

Логируйте:
- событие при получении (start)
- событие после обработки (end) — чтобы получить `duration_ms` / `processing_time_ms`
- событие при ошибке

### Какие поля важны

- `source`: `"kafka"`
- `topic`, `partition`, `offset`
- `message_id` (или key)
- `processing_time_ms` (или `duration_ms`)
- `request_id` (из контекста теста или из headers сообщения)
- `timestamp`

### Как связать с request_id теста

**Вариант 1 (контекст):** `request_id` живёт в contextvar/fixture и доступен обработчику consumer.  
**Вариант 2 (headers/body):** тест публикует `request_id` в сообщении, consumer читает и логирует.

### Где вызывать логгер

- `MQClient` (низкоуровневый слой)
- `consumer_handler` (точка обработки)
- `domain_service` (если бизнес-логика важна для измерений)

### Пример метрики Kafka

```json
{"source":"kafka","topic":"user.created","partition":3,"offset":15421,"message_id":"abc-123","request_id":"4fa82c09-...","duration_ms":35,"timestamp":"2024-01-01T12:00:00Z"}
```

---


## 6. Сбор метрик из RabbitMQ

### Когда и что логировать

Для RabbitMQ полезно логировать метрики **в двух точках**:

1) **Публикация (publish)** — чтобы понимать, сколько занимает отправка и как сообщения маршрутизируются.  
2) **Потребление/обработка (consume/process)** — чтобы измерять время обработки и находить “узкие места” по очередям.

Типовые события:
- `rabbit.publish` — сообщение опубликовано
- `rabbit.consume` — сообщение получено consumer’ом
- `rabbit.processed` — сообщение успешно обработано (или `rabbit.failed` при ошибке)

### Какие поля важны

Рекомендуемые поля для метрики RabbitMQ:

- `source`: `"rabbitmq"`
- `event`: `"publish" | "consume" | "processed" | "failed"`
- `exchange`
- `routing_key`
- `queue` (для consumer)
- `message_id` (если используется; иначе можно логировать `delivery_tag`/`correlation_id`)
- `correlation_id` (стандартное поле AMQP; удобно для корреляции)
- `delivery_tag` (consumer-side идентификатор доставки)
- `redelivered` (флаг повторной доставки)
- `duration_ms` (время publish или processing)
- `request_id` (сквозной id теста/шага)
- `timestamp`

> На практике часто **`correlation_id` = `request_id`** или `request_id` кладётся в headers (например, `x-request-id`).

### Как связать с request_id теста

Варианты корреляции (по приоритету):

1) **AMQP `correlation_id`**: при публикации устанавливать `correlation_id=request_id`.  
2) **Headers**: добавить `headers={"x-request-id": request_id}`.  
3) **Payload**: если формат допускает (и нет рисков PII), хранить `request_id` в теле.

Consumer при получении:
- читает `correlation_id` или `headers["x-request-id"]`
- пишет метрику с найденным `request_id`

### Где вызывать логгер

- `RabbitMQClient` / `RMQClient` — общий слой (publish/consume), лучший вариант для единообразия
- `publisher_service` — если нужно измерять бизнес-времена публикации
- `consumer_handler` / `message_handler` — для измерения времени обработки

### Пример метрики RabbitMQ (publish)

```json
{"source":"rabbitmq","event":"publish","exchange":"events","routing_key":"user.created","message_id":"abc-123","correlation_id":"4fa82c09-...","request_id":"4fa82c09-...","duration_ms":12,"timestamp":"2024-01-01T12:00:00Z"}
```

### Пример метрики RabbitMQ (consume/process)

```json
{"source":"rabbitmq","event":"processed","queue":"user.created.q","delivery_tag":981,"redelivered":false,"message_id":"abc-123","correlation_id":"4fa82c09-...","request_id":"4fa82c09-...","duration_ms":47,"timestamp":"2024-01-01T12:00:00Z"}
```


## 7. Сбор метрик из БД

### Пример: фиксация времени выполнения SQL-запроса

Рекомендуется логировать:
- `source`: `"db"`
- `query_type`: `SELECT/INSERT/UPDATE/DELETE`
- `table` (если можно извлечь безопасно)
- `duration_ms`
- `rowcount`
- `request_id`
- `timestamp`

> Важно: **не логировать** полный SQL с параметрами, если там могут быть PII/секреты.

### Как замерять длительность

#### Вариант A: обёртка над `session.execute()`

```python
import time

def execute_with_metrics(session, stmt, *, request_id: str, logger, table: str | None = None, query_type: str | None = None):
    started = time.perf_counter()
    result = session.execute(stmt)
    duration_ms = int((time.perf_counter() - started) * 1000)

    logger.log({
        "source": "db",
        "query_type": query_type,
        "table": table,
        "duration_ms": duration_ms,
        "rowcount": getattr(result, "rowcount", None),
        "request_id": request_id,
    })
    return result
```

#### Вариант B: SQLAlchemy event listeners

Подходит для централизованного сбора на уровне engine.  
Плюсы: минимально затрагивает бизнес-код.  
Минусы: аккуратно с извлечением `table/query_type` и с прокидыванием `request_id` (нужен contextvar).

### Пример метрики БД

```json
{"source":"db","query_type":"SELECT","table":"users","duration_ms":110,"rowcount":1,"request_id":"4fa82c09-...","timestamp":"2024-01-01T12:00:00Z"}
```

---

## 8. Best Practices

### Единый формат метрик для всех источников

- Одна схема + расширения по источникам.
- Обязательные поля: `timestamp`, `request_id`, `source`, `duration_ms`.
- Уточняющие поля — только по необходимости.

### Хранение

- `metrics.jsonl` рядом с отчётами (Allure/logs), чтобы легко прикладывать артефакты.
- Для CI: сохранять как артефакт пайплайна.
- Для долговременного хранения: S3/Minio.

Рекомендуемая структура:
```
artifacts/
  metrics/
    metrics.jsonl
  allure-results/
  logs/
```

### Генерация request_id

- Генерировать на уровне фикстуры/теста (scope=function).
- Хранить в контексте (contextvar), чтобы API/Kafka/DB клиенты получали его прозрачно.

### Маскирование чувствительных данных

- Не писать токены/секреты/Authorization headers.
- Не писать payload целиком, если там PII.
- Если нужно — писать только `payload_size_bytes`, безопасные идентификаторы, тип операции.

### Интеграция с Allure

- Прикладывать `metrics.jsonl` как attachment (suite/session).
- Дополнительно можно прикладывать агрегаты (например, топ медленных endpoint’ов).

### Параллельные прогоны

- Файл на воркер + merge после прогона — самый надёжный подход.
- File lock — допустим, если среда гарантирует корректные блокировки.

---

## 9. Заключение

Централизованный сбор метрик в автотестах:
- ускоряет диагностику за счёт корреляции по `request_id`;
- повышает наблюдаемость качества и производительности;
- делает CI/CD сравнимым и измеримым.

### Использование метрик для дашбордов

Типовые графики/таблицы:
- p50/p95/p99 `duration_ms` по `endpoint`
- топ медленных тестов (`test_name`)
- частота `http_status >= 500`
- задержка обработки Kafka
- длительность DB-операций и “тяжёлые” таблицы

### Идеи для расширения

- Grafana + Loki/Elastic/ClickHouse
- Prometheus Exporter / Pushgateway (для агрегированных метрик, без высокой кардинальности)
- Автоматический анализ regressions (сравнение p95 между ветками/релизами)

---

## Приложение: рекомендуемая схема полей

Минимальная общая схема:
- `timestamp` (UTC, ISO 8601)
- `request_id`
- `test_name`
- `status` (`passed`/`failed`)
- `environment`
- `source` (`test/api/kafka/db`)
- `duration_ms`

Расширения по источникам:
- API: `method`, `endpoint`, `http_status`
- Kafka: `topic`, `partition`, `offset`, `message_id`
- DB: `query_type`, `table`, `rowcount`
